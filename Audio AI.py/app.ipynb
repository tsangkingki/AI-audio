{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsangkingki/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel,BitsAndBytesConfig,pipeline, is_bitsandbytes_available\n",
    "import torch\n",
    "import numpy\n",
    "from huggingface_hub import InferenceClient,login\n",
    "import accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/tsangkingki/.cache/huggingface/token\n",
      "Login successful\n",
      "NVIDIA GeForce GTX 1660 SUPER\n",
      "Default device: cuda\n"
     ]
    }
   ],
   "source": [
    "api_token =\"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\"\n",
    "login(api_token)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(torch.cuda.get_device_name(device))\n",
    "print(\"Default device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Check of dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitsandbytes 0.43.1\n",
      "transformers 4.42.3\n",
      "numpy 1.26.4\n",
      "accelerate 0.32.1\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import bitsandbytes;\n",
    "print(\"bitsandbytes\",bitsandbytes.__version__)\n",
    "# print(bitsandbytes.__file__)\n",
    "print(\"transformers\",transformers.__version__)\n",
    "print(\"numpy\",numpy.__version__)\n",
    "# print(numpy.__file__)\n",
    "print(\"accelerate\",accelerate.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(is_bitsandbytes_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/tsangkingki/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:480: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:1700\u001b[0m, in \u001b[0;36m_check_class\u001b[0;34m(klass, attr)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: '_torchdynamo_inline'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     text \u001b[38;5;241m=\u001b[39m pipe(url)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m----> 6\u001b[0m ASRoutput \u001b[38;5;241m=\u001b[39m\u001b[43maudioToText\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.mp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m;\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(ASRoutput)\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36maudioToText\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maudioToText\u001b[39m(url):\n\u001b[1;32m      2\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautomatic-speech-recognition\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malvanlii/distil-whisper-small-cantonese\u001b[39m\u001b[38;5;124m\"\u001b[39m,device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 3\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:284\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    223\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    225\u001b[0m ):\n\u001b[1;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1246\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/transformers/pipelines/base.py:1161\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1160\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1161\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:504\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline._forward\u001b[0;34m(self, model_inputs, return_timestamps, **generate_kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m             generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_frames\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_frames\n\u001b[0;32m--> 504\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# whisper longform generation stores timestamps in \"segments\"\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_timestamps \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq2seq_whisper\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:486\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[0;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[1;32m    480\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input name `inputs` is deprecated. Please make sure to use `input_features` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    483\u001b[0m     )\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# 1. prepare generation config\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m generation_config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_generation_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# 2. set global generate variables\u001b[39;00m\n\u001b[1;32m    489\u001b[0m input_stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/transformers/generation/utils.py:1357\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_generation_config\u001b[0;34m(self, generation_config, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;66;03m# TODO joao: when we can detect `fullgraph=True` in `torch.compile` (https://github.com/pytorch/pytorch/pull/120400)\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;66;03m# replace `is_torchdynamo_compiling` by the corresponding check. As it is, we are being too restrictive with\u001b[39;00m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;66;03m# the parameterization in `fullgraph=False` so as to enable `fullgraph=True`.\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m \n\u001b[1;32m   1348\u001b[0m \u001b[38;5;66;03m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;66;03m# legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;66;03m# three conditions must be met\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;66;03m# 3) the user must have set generation parameters in the model config.\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;66;03m# NOTE: `torch.compile` can't compile `hash`, this legacy support is disabled with compilation.\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m-> 1357\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_torchdynamo_compiling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config\n\u001b[1;32m   1359\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_original_object_hash \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config)\n\u001b[1;32m   1360\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_has_non_default_generation_parameters()\n\u001b[1;32m   1361\u001b[0m     ):\n\u001b[1;32m   1362\u001b[0m         new_generation_config \u001b[38;5;241m=\u001b[39m GenerationConfig\u001b[38;5;241m.\u001b[39mfrom_model_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m   1363\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_generation_config \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config:\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/transformers/utils/import_utils.py:668\u001b[0m, in \u001b[0;36mis_torchdynamo_compiling\u001b[0;34m()\u001b[0m\n\u001b[1;32m    666\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdynamo\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dynamo\u001b[38;5;241m.\u001b[39mis_compiling()\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/torch/_dynamo/__init__.py:64\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_builtins\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Wrap manual_seed with the disable decorator.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Can't do it at its implementation due to dependency issues.\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed \u001b[38;5;241m=\u001b[39m \u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Add the new manual_seed to the builtin registry.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_builtins\u001b[38;5;241m.\u001b[39m_register_builtin(torch\u001b[38;5;241m.\u001b[39mmanual_seed, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maten::manual_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/torch/_dynamo/decorators.py:50\u001b[0m, in \u001b[0;36mdisable\u001b[0;34m(fn, recursive)\u001b[0m\n\u001b[1;32m     48\u001b[0m         fn \u001b[38;5;241m=\u001b[39m innermost_fn(fn)\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn)\n\u001b[0;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDisableContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DisableContext()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:410\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 410\u001b[0m     (filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtrace_rules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_impl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_wrapped_call_impl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    413\u001b[0m     )\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m DONT_WRAP_FILES\n\u001b[1;32m    415\u001b[0m ):\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# call to a builtin without a frame for us to capture\u001b[39;00m\n\u001b[1;32m    417\u001b[0m     fn \u001b[38;5;241m=\u001b[39m external_utils\u001b[38;5;241m.\u001b[39mwrap_inline(fn)\n\u001b[1;32m    419\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:3378\u001b[0m, in \u001b[0;36mcheck\u001b[0;34m(obj, is_inlined_call)\u001b[0m\n\u001b[1;32m   3377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(obj, is_inlined_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck_verbose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inlined_call\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mskipped\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:3361\u001b[0m, in \u001b[0;36mcheck_verbose\u001b[0;34m(obj, is_inlined_call)\u001b[0m\n\u001b[1;32m   3358\u001b[0m     fi \u001b[38;5;241m=\u001b[39m FunctionInfo(obj, \u001b[38;5;28;01mNone\u001b[39;00m, getfile(obj), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;66;03m# Consulte the central trace rules defined in torch._dynamo.trace_rules.\u001b[39;00m\n\u001b[0;32m-> 3361\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_rules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup_inner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_inlined_call\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rule \u001b[38;5;129;01min\u001b[39;00m [UserFunctionVariable, FunctorchHigherOrderVariable]:\n\u001b[1;32m   3365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SkipResult(\n\u001b[1;32m   3366\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   3367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minlined according trace_rules.lookup\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3368\u001b[0m     )\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:3442\u001b[0m, in \u001b[0;36mlookup_inner\u001b[0;34m(obj, name, filename, is_direct_call)\u001b[0m\n\u001b[1;32m   3440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_aten_op_or_tensor_method(obj):\n\u001b[1;32m   3441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TorchInGraphFunctionVariable\n\u001b[0;32m-> 3442\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[43mget_torch_obj_rule_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget(obj, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   3443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rule\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:2782\u001b[0m, in \u001b[0;36mget_torch_obj_rule_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m torch_name_rule_map:\n\u001b[1;32m   2781\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m-> 2782\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mload_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2783\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2784\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m d \u001b[38;5;129;01mand\u001b[39;00m d[obj] \u001b[38;5;241m!=\u001b[39m v:\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:2812\u001b[0m, in \u001b[0;36mload_object\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2810\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid obj name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2811\u001b[0m         val \u001b[38;5;241m=\u001b[39m _load_obj_from_str(x[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m-> 2812\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43munwrap_if_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[1;32m   2814\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py:558\u001b[0m, in \u001b[0;36munwrap_if_wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munwrap_if_wrapper\u001b[39m(fn):\n\u001b[0;32m--> 558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munwrap_with_attr_name_if_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/working/AI-audio/Audio AI.py/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py:567\u001b[0m, in \u001b[0;36munwrap_with_attr_name_if_wrapper\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    565\u001b[0m     attr_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__wrapped__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# unpack @torch._dynamo.optimize()(fn) wrapped function\u001b[39;00m\n\u001b[0;32m--> 567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_function(fn) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetattr_static\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_torchdynamo_inline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[1;32m    568\u001b[0m     fn \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetattr_static(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torchdynamo_inline\u001b[39m\u001b[38;5;124m\"\u001b[39m, fn)\n\u001b[1;32m    569\u001b[0m     attr_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torchdynamo_inline\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:1747\u001b[0m, in \u001b[0;36mgetattr_static\u001b[0;34m(obj, attr, default)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     klass \u001b[38;5;241m=\u001b[39m obj\n\u001b[0;32m-> 1747\u001b[0m klass_result \u001b[38;5;241m=\u001b[39m \u001b[43m_check_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mklass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _sentinel \u001b[38;5;129;01mand\u001b[39;00m klass_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _sentinel:\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (_check_class(\u001b[38;5;28mtype\u001b[39m(klass_result), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__get__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _sentinel \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m         _check_class(\u001b[38;5;28mtype\u001b[39m(klass_result), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__set__\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _sentinel):\n",
      "File \u001b[0;32m/usr/lib/python3.10/inspect.py:1700\u001b[0m, in \u001b[0;36m_check_class\u001b[0;34m(klass, attr)\u001b[0m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _shadowed_dict(\u001b[38;5;28mtype\u001b[39m(entry)) \u001b[38;5;129;01mis\u001b[39;00m _sentinel:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1700\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1701\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   1702\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def audioToText(url):\n",
    "    pipe = pipeline(\"automatic-speech-recognition\", model=\"alvanlii/distil-whisper-small-cantonese\",device=device)\n",
    "    text = pipe(url)\n",
    "    return text\n",
    "\n",
    "ASRoutput =audioToText(\"test.mp3\");\n",
    "print(ASRoutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:01<00:00, 15.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "who is lebron james?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeBron James is an American professional basketball player who is widely regarded as one of the greatest players in the history of the National Basketball Association (NBA). He was born on December 30, 1984, in Akron, Ohio, and began playing basketball at a young age.\n",
      "\n",
      "James played high school basketball at St. Vincent-St. Mary High School in Akron, where he won three state championships and was named the Ohio Mr. Basketball three times. He was drafted first overall by the Cleveland Cavaliers in the 2003 NBA draft and has since played for the Miami Heat, Los Angeles Lakers, and Cleveland Cavaliers again.\n",
      "\n",
      "Throughout his career\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def llm(messages):\n",
    "    if not hasattr(llm, \"pipe\"):  # Check if pipeline is already initialized\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,# how to twice\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "        model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            # device_map=\"auto\",\n",
    "            quantization_config=bnb_config,\n",
    "        )\n",
    "        # Load the model and initialize the pipeline\n",
    "        llm.pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, trust_remote_code=True)\n",
    "        \n",
    "    # Prompt template\n",
    "    prompt = llm.pipe.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    print(\"prompt:\", prompt)\n",
    "\n",
    "    # Identify the end of text\n",
    "    terminators = [\n",
    "        llm.pipe.tokenizer.eos_token_id,\n",
    "        llm.pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    response = llm.pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=128,  # Reduced max tokens for faster response\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,  # Text generation strategies\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,  \n",
    "    )\n",
    "\n",
    "    return response[0][\"generated_text\"][len(prompt):]\n",
    "\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"who is lebron james?\"}]\n",
    "llm_output = llm(messages)\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: <|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "中國喺邊?用中文廣東話答<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "你問中國喺邊？（You're asking where China is?）\n",
      "\n",
      "喺地球上，中國喺亞洲大陸上，位於東亞地區。從地理角度講，中國的西界是新疆、青海、甘肅、寧夏、陝西五省，北界是蒙古國、俄羅斯、朝鮮民主主義人民共和國三個國家，東界是東海、黃海、渤海、渤海灣、南海五個海域，南界是南海、南中國海、越南民主共和國\n"
     ]
    }
   ],
   "source": [
    "messages2 = [{\"role\": \"user\", \"content\": \"中國喺邊?用中文廣東話答\"}]\n",
    "llm_output2 = llm(messages2)\n",
    "print(llm_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma AS DB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"./Chroma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collection = client.get_or_create_collection(name=\"test_collection2\",embedding_function=ef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1721000308100838884"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.heartbeat() # returns a nanosecond heartbeat. Useful for making sure the client remains connected.\n",
    "# client.reset() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from openai import AsyncOpenAI,OpenAI\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='1+1 equals 2.', role='assistant', function_call=None, tool_calls=None))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "openai.api_key = \"sk-mkUrszGXaiie7P6HKp3o702vMHP80MDsRnxNDkALUo1TsRFv\"\n",
    "\n",
    "GPTclient = AsyncOpenAI(\n",
    "  api_key=openai.api_key ,  # this is also the default, it can be omitted\n",
    "  base_url=\"https://api.chatanywhere.tech/v1\"\n",
    ")\n",
    "\n",
    "async def gpt(message):\n",
    "    completion = await GPTclient.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": message}])\n",
    "    return completion\n",
    "\n",
    "result = await gpt(\"1+1 = ?\")\n",
    "print(result.choices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeBron James is a professional basketball player who currently plays for the Los Angeles Lakers in the NBA. He is considered one of the greatest basketball players of all time and has won multiple NBA championships and MVP awards throughout his career. LeBron is known for his incredible athleticism, versatility, and basketball IQ. Off the court, he is also involved in various philanthropic activities and business ventures.\n"
     ]
    }
   ],
   "source": [
    "result = await gpt(\"who is lebron james ?\")\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<chromadb.utils.embedding_functions.SentenceTransformerEmbeddingFunction object at 0x7f5e7a8c7e50>\n"
     ]
    }
   ],
   "source": [
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "# Use a Chinese-specific embedding model\n",
    "ef = SentenceTransformerEmbeddingFunction(model_name=\"BAAI/bge-small-zh-v1.5\")\n",
    "print(ef)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pdf to md file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "command = \"marker_single ./pdfFile/fee_general_services_tc.pdf ./pdfOutput --batch_multiplier 2 --max_pages 10 --langs Chinese\"\n",
    "subprocess.run(command.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'Data/fee_general_services_tc.md'}, page_content='2023年4月8日生效 一般服務 - 所有戶口 1.   發出銀行本票\\n\\n港元 i)    私人銀行客戶/VIP 銀行服務客戶 ii)   一般本行客戶 iii)     328 營商理財客戶(晉越級)\\n\\n美元 i)    私人銀行客戶/VIP 銀行服務客戶 ii)   一般本行客戶 iii)     328 營商理財客戶(晉越級)\\n\\n保兌本行本票(港元 /美元)\\n\\n回購銀行本票(港元 /美元)\\n\\n本行客戶 4.    報失銀行本票\\n\\n港元\\n\\n美元 5.   禮券*\\n\\n私人銀行客戶/VIP 銀行服務客戶\\n\\ni-Account客戶\\n\\n一般本行客戶\\n\\n非本行客戶 6.   旅行支票\\n\\n兌現 i)  私人銀行客戶/VIP 銀行服務客戶 ii)   一般本行客戶\\n\\n存入戶口 7.    輔幣找換\\n\\ni)   私人銀行客戶/VIP 銀行服務客戶/一般本行客戶 ii)    328 營商理財客戶(晉越級)\\n\\n大量港元輔幣存款\\n\\n每位客戶每日存入100枚或以下\\n\\n每位客戶每日存入100枚以上 i)   私人銀行客戶/VIP銀行服務客戶/一般本行客戶 ii)     328 營商理財客戶(晉越級)\\n\\n大量紙幣存款\\n\\n港元\\n\\n每位客戶每日存入200張或以下\\n\\n每位客戶每日存入200張以上 i)   私人銀行客戶/VIP銀行服務客戶/一般本行客戶 外幣 ii)  328 營商理財客戶(晉越級)\\n\\n每位客戶每日存入100張或以下\\n\\n每位客戶每日存入100張以上 i)   私人銀行客戶/VIP銀行服務客戶/一般本行客戶 ii)     328 營商理財客戶(晉越級)\\n\\n大量港元支票存入\\n\\n每日存入30張或以下\\n\\n每日存入30張以上 i)   私人銀行客戶/VIP銀行服務客戶/一般本行客戶 ii)     328 營商理財客戶(晉越級)\\n\\n外幣現鈔兌換\\n\\n本行客戶\\n\\n非本行客戶 12. 代匯水佣金 i)     私人銀行客戶/VIP 銀行服務客戶 ii)   一般本行客戶 13. 提款卡年費(65 歲或以上豁免收費)\\n\\n大新港幣卡 i)   私人銀行客戶/VIP銀行服務客戶/一般本行客戶 ii)     328 營商理財客戶(晉越級)\\n\\n大新人民幣卡\\n\\n易理財卡\\n\\n大新i-Account卡+\\n14. 補發提款卡(65 歲或以上豁免收費)\\n\\n大新港幣卡\\n\\n大新人民幣卡\\n\\n易理財卡\\n\\n補發兒童儲蓄戶口存款卡 15.    於香港經「銀聯」自動櫃員機網絡由 港幣戶口提取現金 16.   於香港以外地區經「銀聯」自動櫃員機網絡 由港幣戶口提取現金Δ#\\n17. 於香港以外地區經「銀聯」自動櫃員機網絡 由人民幣戶口提取現金Δ#\\n18. 於香港以外地區經「銀通」自動櫃員機網絡 提取現金Δ##\\n19. 在貼有「跨行轉賬」標誌的「銀通」自動櫃員機 進行跨行轉賬 20.    申領本月尚未發出之結單\\n\\n(包括透過自動櫃員機之申領)\\n\\n港元 /其他外幣\\n\\n人民幣 21.    a)  申領銀行月結單影印本 i)    私人銀行客戶/VIP銀行服務客戶/一般本行客戶\\n\\n(只適用於大新網上理財客戶)\\n\\n港元\\n\\n美元\\n\\n人民幣 ii)     私人銀行客戶/VIP銀行服務客戶/一般本行客戶\\n\\n港元\\n\\n美元\\n\\n人民幣 iii)    328 營商理財客戶(晉越級)\\n\\n港元\\n\\n美元\\n\\n人民幣 b) 申領已付支票 /通知書影印本\\n\\n港元\\n\\n美元\\n\\n人民幣 22.    申領儲蓄 /定期戶口過往紀錄\\n\\n申請日起一年內\\n\\n申請日起兩年內\\n\\n申請日起三年內\\n\\n若超過三年,每年計(最多至七年)\\n\\n申領個人資料紀錄\\n\\n查閱資料 24.  核數證明 i)  私人銀行客戶/VIP 銀行服務客戶/一般本行客戶 ii)    328 營商理財客戶(晉越級)\\n\\n賬戶結餘證明書 26. 銀行介紹信 i)     私人銀行客戶/VIP 銀行服務客戶/一般本行客戶 ii)    328 營商理財客戶(晉越級)\\n\\n領事館簽證申請信上加簽印鑑 28.   銀行按時付款服務\\n\\n週期支付(每次交易)\\n\\n以本票 i)  私人銀行客戶/VIP 銀行服務客戶 ii)   一般本行客戶\\n\\n經匯款 i)  私人銀行客戶/VIP 銀行服務客戶\\n\\n修改 ii)     一般本行客戶\\n\\n暫停指示\\n\\n存款不足 29.   自動轉賬付款(一般客戶)\\n\\n首次訂立指示 30.    自動轉賬收 /付款(商業客戶)\\n\\n首次訂立指示\\n\\n過賬收費 列表指示\\n\\n大新銀行戶口\\n\\n其他銀行戶口 電腦可讀輸入指示\\n\\n大新銀行戶口\\n\\n其他銀行戶口 328營商網上理財指示\\n\\n大新銀行戶口\\n\\n其他銀行戶口 31. 保險箱\\n\\n遺失一條鎖匙\\n\\n鑿箱費用(包括2條新匙)\\n\\n代存郵件服務 33.  提早贖回定期存款 /到期日前終止\\n\\n「夢想成真」儲蓄計劃\\n\\n提早贖回定期存款\\n\\n到期日前終止「夢想成真」儲蓄計劃\\n\\n開戶後三個月內取消戶口\\n\\n開戶三個月後取消戶口 34. 申領有關存款保障計劃之信件 35.   寄回退票(掛號郵寄)\\n\\n商業登記或有限公司查冊費\\n\\n港元戶口\\n\\n美元戶口 37.   開立香港註冊公司戶口之費用 38.   開立海外註冊公司戶口之費用 39.   ~額外費用 若有關公司經查冊後被界定為特殊公司包括 但不限於以下種類:\\n\\n具三層或以上擁有權/控制權結構之公司,或\\n\\n從事貨幣兌換、匯款或匯款代理服務,或\\n\\n信託賬戶,或\\n\\n與虛擬商品相關的業務,例如經營虛擬商品 交易、經紀、銷售或購買虛擬商品 40.    購買Gift卡 41. Gift卡服務月費 42.   郵寄月結單費用^\\n\\n私人銀行客戶\\n\\n個人客戶 免費 每張50港元 每張25港元 免費 每張7美元 每張3.5美元 每張50港元 每張 50 港元 /7 美元 每張400港元 每張52美元 免費(此服務只限經戶口辦理)\\n\\n免費(此服務只限經戶口辦理)\\n\\n每張10港元(此服務只限經戶口辦理)\\n\\n不再提供此項服務 每次兌現總額之0.375%(最低50港元)\\n\\n每次兌現總額之0.375%(最低100港元)\\n\\n存入總額之 0.375%(最低 50 港元)\\n\\n每包5港元 每包2.5港元 免費 每包 5 港元(以50枚為一包計,不足 一包將作一包計,最低50港元)\\n\\n每包 2.5 港元(以50枚為一包計,不足 一包將作一包計,最低25港元)\\n\\n免費 紙幣總額之 0.5%(最低 150 港元)\\n\\n紙幣總額之 0.25%(最低 75 港元)\\n\\n免費 紙幣總額之0.5%(最低150港元或等值)\\n\\n紙幣總額之0.25%(最低 75港元或等值)\\n\\n免費 超出30張以上之張數計每張2港元 超出30張以上之張數計每張1港元 免費(此服務只限經戶口兌換)\\n\\n不再提供此項服務 i)  若提取/存入** 之外幣現鈔(I)金額 多於以下列表所列明的特定金額 或(II)並非以下列表之外幣,本行 將會收取以外幣兌港幣現鈔匯率及 外幣電匯匯率之差價作為代匯水 佣金(最低 50 港元):\\n\\n5,000美元  5,000加元 5,000澳元     3,000英鎊 5,000紐元     3,000歐羅 500,000日元 ii) 外幣現鈔存入 外幣儲蓄戶口/\\n\\n由外幣儲蓄戶口\\n\\n提取外幣現鈔,\\n\\n本行以外幣兌港幣現鈔匯率及 外幣電匯匯率之差價作為代匯水 佣金(最低 50 港元)\\n\\n50港元 豁免(只適用於 328 商業理財戶口)\\n\\n人民幣50元 50港元 豁免(只適用於綜合理財戶口)\\n\\n50港元 人民幣50元 50港元 50港元 每次20港元 每次50港元 每次人民幣50元 每次28港元 每次10港元 每份50港元或等值 每份人民幣50元 豁免 豁免 豁免 每份50港元 每份6美元 每份人民幣50元 每份25港元 每份3美元 每份人民幣25元 每頁50港元 每頁6美元 每頁人民幣50元 每戶口250港元 每戶口750港元 每戶口1,000港元 每戶口每年加收1,000港元 每次200港元 每份350港元 每份175港元 每張150港元 每封300港元 每封150港元 每張150港元 每項50港元 每項100港元另加本票費用 每項50港元另加匯款費用 每項100港元另加匯款費用 每項100港元 每項70港元 每指示150港元 豁免 豁免 每項 2 港元(最低 100 港元)\\n\\n每項 5 港元(最低 100 港元)\\n\\n每項 0.3 港元(最低 50 港元)\\n\\n每項 1.5 港元(最低 50 港元)\\n\\n豁免 每項 1.5 港元(最低 50 港元)\\n\\n300港元 1,200港元 不再提供此項服務 港元 基準利率(港元最優惠利率ΔΔ 或 合約利率 + 2%存款年利率\\n\\n(以較高者為準)﹣合約利率)x 存款本金 x 到期日尚餘日數 ÷ 365日 或 最低200港元 外幣 2%存款年利率 x 存款本金 x 到期日尚餘日數 ÷ 365日 或 最低200港元 ΔΔ 港元最優惠利率根據大新銀行有限公司 不時之決定而釐定 每個戶口200港元 客戶將不獲得任何利息 豁免 存款利息將按本行之港元通知存款 利率++計算 每份100港元 每張25港元/3美元/人民幣20元 加非本地郵費(如適用)\\n\\n每項200港元 每項19美元 每次申請1,200港元; 另加額外費用~(如適用)\\n\\n每次申請10,000港元; 另加額外費用~(如適用)\\n\\n每次申請10,000港元 每張10港元 當有效期限屆滿後,如Gift卡仍有 任何剩餘儲存價值,銀行可由有效 期限屆滿後起計第二個月開始,徵收 服務月費25港元 豁免 如戶口基本持有人於每年4月至9月或 每年10月至翌年3月期間收取1張或 以上郵寄月結單,本行將就每個戶口 於該期間之郵寄月結單收取20港元或 等值,並分別於同年10月或翌年4月 誌賬。此收費將於該戶口內扣除。\\n\\n以下客戶將可獲豁免此收費:\\n\\n(1)  65歲或以上之客戶;或\\n\\n(2)  領取政府綜援或社會福利署津貼 之客戶(需提供証明文件)^^;或\\n\\n(3) 領取政府傷殘津貼之客戶(需提供 証明文件)^^;或\\n\\n(4)  低收入人士之客戶,即個人月入 低於 7,300 港元或家庭月入低於 11,500港元(需提供証明文件)^^;\\n\\n(5)  持有證券服務之客戶;或\\n\\n(6) VIP銀行服務/Hello Kitty VIP 銀行 服務客戶於收費日前一個月內 之全面綜合理財總值^^^ 達 1,000,000港元或以上(或等值)。\\n\\n如為聯名存款戶口,只要任何一位戶口 持有人屬於豁免類別之人士,其戶口 可獲豁免收費。\\n\\n「長者咭」持有人可獲豁免此收費。\\n\\n**   如客戶欲存入指定外幣之現鈔至外幣儲蓄戶口,請於指定之分行辦理。 查詢詳情,請瀏覽本行網頁。\\n\\n包括大新八達通VIP App卡、大新八達通優易理財App卡、大新八達通i-Account App卡、VIP銀行服務卡、優易 理財卡、i-Account卡、Hello Kitty VIP銀行服務卡、Hello Kitty i-Account卡及Doraemon i-Account卡。\\n\\nΔ 提供自動櫃員機服務的當地銀行或會就提取現金收取費用。提款前請留意。\\n\\n# 經「銀聯」自動櫃員機網絡提取現金,所提取貨幣將以「銀聯」訂定的當日兌換率直接進行兌換,客戶可瀏覽「銀聯」\\n\\n網站www.unionpayintl.com查詢當日兌換率。\\n\\n## 經「銀通」自動櫃員機網絡提取現金,所提取的澳門幣將按「銀通」訂定的當日兌換率直接進行兌換,客戶可致電 銀通客戶服務熱線2520 1747查詢有關匯率,而提取的人民幣將按提供自動櫃員機服務的銀行訂定的當日兌換率直接 進行兌換。\\n\\n++ 港元通知存款利率詳情請參閱本行網頁。\\n\\n^ 此郵寄月結單收費只適用於個人存款戶口:包括往來戶口、儲蓄戶口及綜合理財戶口(不包括兒童儲蓄戶口、備用\\n\\n「快應錢」及按揭 1+1 戶口)。\\n\\n^^   符合上述(2),(3)或(4)資格的客戶須主動通知本行並提供有關証明以申請豁免是項費用。\\n\\n^^^ 全面綜合理財總值包括於本行持有的存款戶口之存款結餘、投資戶口之最新市值及人壽保險戶口之參考保費。參考 保費是指本行按其指定第三方承保方(如適用)所提供最新有效人壽保單資料作出運算,所計算的保費可能會與實際 已繳之累積保費不同,其中並不包括預繳保費、保費優惠及保單貸款等。於計算全面綜合理財總值時,非港元的 保單之參考保費會以預先釐定的匯率折算為港元等值計算。詳情請瀏覽大新銀行網站 www.dahsing.com 或與分行 職員聯絡。\\n\\n一般本行客戶是指本行個人客戶及非個人客戶,除非另有定明。\\n\\n有關328營商理財客戶(晉越級)之定義及詳情,請瀏覽銀行網頁 www.dahsing.com/biz/reward。\\n\\n如私人銀行客戶有任何查詢,請與您的私人銀行客戶經理聯絡。\\n\\n如中文譯本之文義與英文原本有歧異時,應以英文原本為準。\\n\\n以上收費如為港元等值,將按照本行認為適當之外幣兌換率收取。代付費用(如代理銀行費用)均由客戶承擔。收費未能 盡錄,詳情請向各分行/部門查詢或瀏覽 www.dahsing.com。大新銀行保留一切權利隨時調整任何收費優惠或附加其他 銀行收費。如閣下有任何疑問,請致電本銀行客戶服務熱線:\\n\\n2828 8000(個人銀行客戶) 2828 7028(證券客戶)\\n\\n2828 8008(328營商理財客戶)     2828 8009(現金卡客戶)\\n\\n2507 6000(汽車及機器貸款客戶)     2828 5488(信用卡客戶)')]\n"
     ]
    }
   ],
   "source": [
    "#load data \n",
    "DATA_PATH = \"Data\"\n",
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "docs=load_documents()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    # Assign unique string IDs to each chunk\n",
    "    for chunk in chunks:\n",
    "        chunk.id = str(uuid.uuid4())\n",
    "    return chunks\n",
    "chunks = split_text(docs)\n",
    "# Extract the IDs of each chunk into an array\n",
    "chunk_ids = [chunk.id for chunk in chunks]\n",
    "page_contents = [chunk.page_content for chunk in chunks]\n",
    "meta_data = [chunk.metadata for chunk in chunks]\n",
    "print(chunks)\n",
    "print(chunk_ids)\n",
    "print(page_contents)\n",
    "print(meta_data)\n",
    "# collection.upsert(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save documents to db:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collection.upsert(\n",
    "    documents=page_contents,\n",
    "    ids= chunk_ids,\n",
    "    # embeddings=embeddings.tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting & query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def querying(your_messages):\n",
    "    results = test_collection.query(\n",
    "        query_texts=your_messages,\n",
    "        n_results=3 # how many results to return\n",
    "    )\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc for doc in results[\"documents\"][0]])\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=your_messages)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "Answer the question based only on the following context:\n",
      "\n",
      "外幣現鈔兌換\n",
      "\n",
      "本行客戶\n",
      "\n",
      "非本行客戶 12. 代匯水佣金 i)     私人銀行客戶/VIP 銀行服務客戶 ii)   一般本行客戶 13. 提款卡年費(65 歲或以上豁免收費)\n",
      "\n",
      "大新港幣卡 i)   私人銀行客戶/VIP銀行服務客戶/一般本行客戶 ii)     328 營商理財客戶(晉越級)\n",
      "\n",
      "大新人民幣卡\n",
      "\n",
      "易理財卡\n",
      "\n",
      "大新i-Account卡+\n",
      "14. 補發提款卡(65 歲或以上豁免收費)\n",
      "\n",
      "大新港幣卡\n",
      "\n",
      "大新人民幣卡\n",
      "\n",
      "易理財卡\n",
      "\n",
      "---\n",
      "\n",
      "外幣現鈔兌換\n",
      "\n",
      "本行客戶\n",
      "\n",
      "非本行客戶 12. 代匯水佣金 i)     私人銀行客戶/VIP 銀行服務客戶 ii)   一般本行客戶 13. 提款卡年費(65 歲或以上豁免收費)\n",
      "\n",
      "大新港幣卡 i)   私人銀行客戶/VIP銀行服務客戶/一般本行客戶 ii)     328 營商理財客戶(晉越級)\n",
      "\n",
      "大新人民幣卡\n",
      "\n",
      "易理財卡\n",
      "\n",
      "大新i-Account卡+\n",
      "14. 補發提款卡(65 歲或以上豁免收費)\n",
      "\n",
      "大新港幣卡\n",
      "\n",
      "大新人民幣卡\n",
      "\n",
      "易理財卡\n",
      "\n",
      "---\n",
      "\n",
      "大新人民幣卡\n",
      "\n",
      "易理財卡\n",
      "\n",
      "大新i-Account卡+\n",
      "14. 補發提款卡(65 歲或以上豁免收費)\n",
      "\n",
      "大新港幣卡\n",
      "\n",
      "大新人民幣卡\n",
      "\n",
      "易理財卡\n",
      "\n",
      "補發兒童儲蓄戶口存款卡 15.    於香港經「銀聯」自動櫃員機網絡由 港幣戶口提取現金 16.   於香港以外地區經「銀聯」自動櫃員機網絡 由港幣戶口提取現金Δ#\n",
      "17. 於香港以外地區經「銀聯」自動櫃員機網絡 由人民幣戶口提取現金Δ#\n",
      "18. 於香港以外地區經「銀通」自動櫃員機網絡 提取現金Δ##\n",
      "19. 在貼有「跨行轉賬」標誌的「銀通」自動櫃員機 進行跨行轉賬 20.    申領本月尚未發出之結單\n",
      "\n",
      "(包括透過自動櫃員機之申領)\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: 我70 歲我的香港大新銀行提款卡年費幾多?\n",
      "\n",
      "根據上文提供的信息，您70歲，香港大新銀行提款卡年費將免收。\n"
     ]
    }
   ],
   "source": [
    "your_messages = \"我70 歲我的香港大新銀行提款卡年費幾多?\"\n",
    "prompt=await querying(your_messages)\n",
    "print(prompt)\n",
    "result = await gpt(prompt)\n",
    "print(result.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
